{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcc169e",
   "metadata": {},
   "source": [
    "# üß™ LoRA & Fine-Tuning Benchmark Suite\n",
    "\n",
    "**Objective**: Compare LoRA, QLoRA, and other fine-tuning methods on Banking77 intent classification.\n",
    "\n",
    "**Duration**: ~2-3 hours per full run\n",
    "\n",
    "**GPU Required**: Yes (T4 or better)\n",
    "\n",
    "Run cells sequentially. Save results to your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795ad3b",
   "metadata": {},
   "source": [
    "## üì¶ Part 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3893ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional, for saving results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec223c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q peft transformers datasets evaluate tensorboard bitsandbytes scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c747b",
   "metadata": {},
   "source": [
    "## üì• Part 2: Load Banking77 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae11e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Banking77 dataset\n",
    "print(\"üì• Loading Banking77 dataset...\")\n",
    "dataset = load_dataset(\"banking77\")\n",
    "\n",
    "# Inspect dataset\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(dataset[\"train\"][0])\n",
    "print(f\"\\nNumber of intent classes: {dataset['train'].features['label'].num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feab5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test\n",
    "train_val = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test = train_val[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_val[\"train\"],\n",
    "    \"validation\": val_test[\"train\"],\n",
    "    \"test\": val_test[\"test\"],\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec94558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "print(\"üîß Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenization complete!\")\n",
    "print(f\"Sample tokenized: {tokenized_dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12847029",
   "metadata": {},
   "source": [
    "## üî¨ Part 3: Benchmark LoRA with Different Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configurations to test\n",
    "lora_configs = [\n",
    "    {\"r\": 4, \"lora_alpha\": 8, \"lora_dropout\": 0.05, \"name\": \"LoRA-r4\"},\n",
    "    {\"r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05, \"name\": \"LoRA-r8\"},\n",
    "    {\"r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.1, \"name\": \"LoRA-r16\"},\n",
    "    {\"r\": 32, \"lora_alpha\": 64, \"lora_dropout\": 0.1, \"name\": \"LoRA-r32\"},\n",
    "]\n",
    "\n",
    "results = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"üöÄ Starting LoRA benchmarks on {device.upper()}\\n\")\n",
    "\n",
    "for i, config in enumerate(lora_configs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmark {i+1}/{len(lora_configs)}: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load base model\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=77,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Count parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    param_ratio = (trainable / total) * 100\n",
    "    \n",
    "    print(f\"Parameters: {trainable:,} trainable / {total:,} total ({param_ratio:.2f}%)\")\n",
    "    \n",
    "    # Setup trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./lora_{config['name']}\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    )\n",
    "    \n",
    "    # Train with timing\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"Evaluating...\")\n",
    "    predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    accuracy = accuracy_score(dataset[\"test\"][\"label\"], preds)\n",
    "    f1 = f1_score(dataset[\"test\"][\"label\"], preds, average=\"weighted\")\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"method\": \"LoRA\",\n",
    "        \"rank\": config[\"r\"],\n",
    "        \"lora_alpha\": config[\"lora_alpha\"],\n",
    "        \"dropout\": config[\"lora_dropout\"],\n",
    "        \"trainable_params\": trainable,\n",
    "        \"total_params\": total,\n",
    "        \"param_ratio\": param_ratio,\n",
    "        \"training_time_sec\": training_time,\n",
    "        \"peak_memory_gb\": peak_memory,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Complete!\")\n",
    "    print(f\"   Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"   F1 Score: {f1:.4f}\")\n",
    "    print(f\"   Time: {training_time:.1f}s\")\n",
    "    print(f\"   Memory: {peak_memory:.1f}GB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ All benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3485f4c",
   "metadata": {},
   "source": [
    "## üìä Part 4: Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(\"rank\")\n",
    "\n",
    "print(\"\\nüìã Benchmark Results:\")\n",
    "print(results_df[[\"rank\", \"accuracy\", \"f1_score\", \"training_time_sec\", \"peak_memory_gb\", \"param_ratio\"]].to_string())\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"lora_benchmark_results.csv\", index=False)\n",
    "print(\"\\nüíæ Saved to lora_benchmark_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"LoRA Benchmark Results - Banking77\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Plot 1: Accuracy vs Rank\n",
    "axes[0, 0].plot(results_df[\"rank\"], results_df[\"accuracy\"] * 100, marker=\"o\", linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel(\"LoRA Rank\", fontsize=11)\n",
    "axes[0, 0].set_ylabel(\"Accuracy (%)\", fontsize=11)\n",
    "axes[0, 0].set_title(\"Accuracy vs LoRA Rank\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(results_df[\"rank\"])\n",
    "\n",
    "# Plot 2: Memory vs Rank\n",
    "axes[0, 1].plot(results_df[\"rank\"], results_df[\"peak_memory_gb\"], marker=\"s\", linewidth=2, markersize=8, color=\"orange\")\n",
    "axes[0, 1].set_xlabel(\"LoRA Rank\", fontsize=11)\n",
    "axes[0, 1].set_ylabel(\"Peak Memory (GB)\", fontsize=11)\n",
    "axes[0, 1].set_title(\"Memory Usage vs LoRA Rank\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(results_df[\"rank\"])\n",
    "\n",
    "# Plot 3: Training Time vs Rank\n",
    "axes[1, 0].plot(results_df[\"rank\"], results_df[\"training_time_sec\"], marker=\"^\", linewidth=2, markersize=8, color=\"green\")\n",
    "axes[1, 0].set_xlabel(\"LoRA Rank\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Training Time (seconds)\", fontsize=11)\n",
    "axes[1, 0].set_title(\"Training Time vs LoRA Rank\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(results_df[\"rank\"])\n",
    "\n",
    "# Plot 4: Parameter Efficiency\n",
    "axes[1, 1].bar(results_df[\"rank\"].astype(str), results_df[\"param_ratio\"], color=\"purple\", alpha=0.7)\n",
    "axes[1, 1].set_xlabel(\"LoRA Rank\", fontsize=11)\n",
    "axes[1, 1].set_ylabel(\"Trainable Parameters (%)\", fontsize=11)\n",
    "axes[1, 1].set_title(\"Parameter Efficiency\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"benchmark_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved as benchmark_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d0bbd",
   "metadata": {},
   "source": [
    "## üîç Part 5: Analysis & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä BENCHMARK ANALYSIS\\n\")\n",
    "\n",
    "# Find best accuracy\n",
    "best_acc_idx = results_df[\"accuracy\"].idxmax()\n",
    "best_acc = results_df.loc[best_acc_idx]\n",
    "print(f\"ü•á Best Accuracy: Rank {best_acc['rank']} with {best_acc['accuracy']:.2%}\")\n",
    "\n",
    "# Find best efficiency (accuracy per MB)\n",
    "results_df[\"efficiency\"] = results_df[\"accuracy\"] / (results_df[\"peak_memory_gb\"] * 1024)\n",
    "best_eff_idx = results_df[\"efficiency\"].idxmax()\n",
    "best_eff = results_df.loc[best_eff_idx]\n",
    "print(f\"‚ö° Best Efficiency: Rank {best_eff['rank']} with {best_eff['accuracy']:.2%} accuracy at {best_eff['peak_memory_gb']:.1f}GB\")\n",
    "\n",
    "# Find fastest training\n",
    "fastest_idx = results_df[\"training_time_sec\"].idxmin()\n",
    "fastest = results_df.loc[fastest_idx]\n",
    "print(f\"üöÄ Fastest Training: Rank {fastest['rank']} in {fastest['training_time_sec']:.1f}s\")\n",
    "\n",
    "# Memory optimization\n",
    "min_mem = results_df[\"peak_memory_gb\"].min()\n",
    "max_mem = results_df[\"peak_memory_gb\"].max()\n",
    "print(f\"\\nüíæ Memory Range: {min_mem:.1f}GB - {max_mem:.1f}GB\")\n",
    "\n",
    "# Accuracy improvement\n",
    "min_acc = results_df[\"accuracy\"].min()\n",
    "max_acc = results_df[\"accuracy\"].max()\n",
    "acc_gain = (max_acc - min_acc) * 100\n",
    "print(f\"üìà Accuracy Range: {min_acc:.2%} - {max_acc:.2%} (gain: {acc_gain:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ RECOMMENDATION:\")\n",
    "print(f\"   Use Rank {best_eff['rank']} for best balance of accuracy and efficiency\")\n",
    "print(f\"   - Achieves {best_eff['accuracy']:.2%} accuracy\")\n",
    "print(f\"   - Uses only {best_eff['peak_memory_gb']:.1f}GB GPU memory\")\n",
    "print(f\"   - Trains in {best_eff['training_time_sec']:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741f5fd",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Part 6: Vector Database Comparison (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5dbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vector DB packages\n",
    "!pip install -q faiss-cpu chromadb sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create test data\n",
    "banking_queries = [\n",
    "    \"I want to transfer money to my account\",\n",
    "    \"What's my account balance\",\n",
    "    \"How do I set up a loan\",\n",
    "    \"Check my recent transactions\",\n",
    "    \"I need to report a fraud\",\n",
    "] * 200  # Repeat to get 1000 queries\n",
    "\n",
    "print(f\"Generating {len(banking_queries)} embeddings...\")\n",
    "embeddings = embedding_model.encode(banking_queries, show_progress_bar=True)\n",
    "print(f\"‚úÖ Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark FAISS\n",
    "import faiss\n",
    "\n",
    "print(\"\\nüî¨ Benchmarking FAISS...\")\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Indexing\n",
    "start = time.time()\n",
    "index.add(embeddings.astype(np.float32))\n",
    "indexing_time = time.time() - start\n",
    "\n",
    "# Query\n",
    "queries = embeddings[0:10].astype(np.float32)\n",
    "start = time.time()\n",
    "distances, indices = index.search(queries, k=5)\n",
    "query_time = (time.time() - start) / len(queries) * 1000  # ms\n",
    "\n",
    "print(f\"‚úÖ FAISS Results:\")\n",
    "print(f\"   Indexing: {indexing_time:.2f}s for {len(embeddings):,} vectors\")\n",
    "print(f\"   Query: {query_time:.1f}ms per query\")\n",
    "print(f\"   Memory: ~{embeddings.nbytes / 1024 / 1024:.1f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4e8c5",
   "metadata": {},
   "source": [
    "## üíæ Save Results to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files to Google Drive\n",
    "import shutil\n",
    "\n",
    "drive_path = \"/content/drive/MyDrive/Banking_LLM_Benchmarks\"\n",
    "!mkdir -p \"{drive_path}\"\n",
    "\n",
    "# Copy results\n",
    "!cp lora_benchmark_results.csv \"{drive_path}/\"\n",
    "!cp benchmark_results.png \"{drive_path}/\"\n",
    "\n",
    "print(f\"‚úÖ Results saved to Drive: {drive_path}\")\n",
    "print(\"   - lora_benchmark_results.csv\")\n",
    "print(\"   - benchmark_results.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
