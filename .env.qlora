# Configuration for QLoRA Fine-tuned Model with vLLM

# Model selection
LLM_MODEL_VERSION=qlora_vllm_finetuned_model_v1.0.0

# Benchmarking
ENABLE_BENCHMARKING=true
BENCHMARK_INTERVAL_REQUESTS=50

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/qlora_model.log

# Description
# Uses QLoRA-fine-tuned Llama 3.1-8B with vLLM inference
# Best for: Production deployment with high accuracy
# Latency: ~10-20ms per token (GPU)
# Accuracy: 95%+ (measured on Banking77)
# Device: GPU (T4 or better)
# Memory: 4-bit quantization, ~5GB
# Cost: $0.50-1.00/hour on cloud GPU
